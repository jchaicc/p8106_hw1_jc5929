---
title: "h1"
output: github_document
date: "2023-02-20"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(tidyverse)
library(caret)
library(glmnet)
library(pls)
```

```{r}
train_data=read_csv("housing_training.csv") %>%
  janitor::clean_names()
test_data=read_csv("housing_test.csv")%>%
  janitor::clean_names()

train_data = na.omit(train_data)
train_data
x_train = model.matrix(sale_price ~., train_data)[,-1]
y_train =train_data$sale_price
test_data = na.omit(test_data)
x_test = model.matrix(sale_price ~., test_data)[,-1]
y_test = test_data$sale_price

myCtrl = trainControl(method = "cv")
```

```{r problem-a}
set.seed(2023)
lm.fit = train(
  x = x_train,
  y = y_train,
  method = "lm",
  trControl = myCtrl
)
summary(lm.fit)
lm.predict <- predict(lm.fit, newdata = x_test)
lm.mse = mean((y_test - lm.predict)^2)
```


```{r problem-2}
set.seed(2023)
myCtrl_1 = trainControl(
  method = "cv",selectionFunction = "oneSE"
)
lasso_fit <- train(
  x = x_train, 
  y = y_train,
  method = "glmnet",
  tuneGrid = expand.grid(
    alpha = 1,
    lambda = exp(seq(8, -2, length = 250))
  ),
  trControl = myCtrl_1
)

lasso.min.fit <- train(
  x = x_train, 
  y = y_train,
  method = "glmnet",
  tuneGrid = expand.grid(
    alpha = 1,
    lambda = exp(seq(8, -2, length = 250))
  ),
  trControl = myCtrl
)
  

coef(lasso_fit$finalModel, lasso_fit$bestTune$lambda)
lasso_fit$bestTune
lasso_predict <- predict(lasso_fit,newdata = x_test)
lasso_mse <- mean((y_test - lasso_predict)^2)

lasso_minpredict <- predict(lasso.min.fit,newdata = x_test)
lasso.min.mse <- mean((y_test - lasso_minpredict)^2)
```
 30 out of 40 are reported
 

```{r problem-3}
set.seed(2023)
enet.fit <- train(
  x = x_train, 
  y = y_train,
  method = "glmnet",
  tuneGrid = expand.grid(
    alpha = seq(0, 1, length = 21),
    lambda = exp(seq(7, -1, length=250))
  ),
  trControl = myCtrl
)
myCol<- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
                    superpose.line = list(col = myCol))
plot(enet.fit, par.settings = myPar)
enet.fit$bestTune$alpha
enet.fit$bestTune$lambda
enet.predict <- predict(enet.fit,newdata = x_test)
enet.mse <- mean((y_test - enet.predict)^2)
```

The selected tuning parameters are $\alpha$=`r enet.fit$bestTune$alpha`  and $\lambda$=`r enet.fit$bestTune$lambda`. And the test error of this model is `r round(enet.mse)`.

```{r problem-4-plsr}
set.seed(2023)
pfit <- plsr(sale_price~., 
                data = train_data, 
                scale = TRUE,  
                validation = "CV")
summary(pfit)
validationplot(pfit, val.type="MSEP", legendpos = "topright")
cv.mse <- RMSEP(pfit)
ncomp.cv <- which.min(cv.mse$val[1,,])-1
ppredict <- predict(pfit, newdata = test_data, ncomp = ncomp.cv)
pls.mse = mean((y_test - ppredict)^2)
```

The model result shows that there are `r ncomp.cv` components included in this model.

we can see that the `lasso_1se` model has the lowest mean test error.
```{r summary_plot}
lm_test_err = (lm.predict-y_test)^2
test_len = length(lm_test_err)
lasso_min_test_err = (lasso_minpredict-y_test)^2
lasso_1se_test_err = (lasso_predict-y_test)^2
enet_test_err = (enet.predict-y_test)^2
pls_test_err = (ppredict-y_test)^2

summary_tab <-
  tibble(
    error = c(lm_test_err, lasso_min_test_err, lasso_1se_test_err, enet_test_err, pls_test_err),
    model = c(rep("lm", test_len),rep("lasso_min",test_len),rep("lasso_1se",test_len),rep("enet",test_len),rep("pls",test_len))
  )
ggplot(data = summary_tab,aes(x = factor(model, level = c("lm", "lasso_min", "lasso_1se", "enet", "pls")), y = error, color = model))+
  geom_boxplot()+
  scale_y_continuous(trans = "log")+
  xlab("Models")+
  ylab("Test Error")
```

```{r summary_table}
summary_tab %>% 
  group_by(model) %>% 
  summarize(
    Min = min(error),
    Q_25 = quantile(error,probs = 0.25),
    Median = median(error),
    MSE = mean(error),
    Q_75 = quantile(error, probs = 0.75),
    Max = max(error)
  ) %>% 
  knitr::kable()
```
considering the test MSE, I would like to choose the LASSO model using the 1SE rule for predicting the response.